{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import jsonlines\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import sentence_splitter\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "datafiles = []\n",
    "for dirname, _, filenames in os.walk('/cord19'):\n",
    "    for filename in filenames:\n",
    "        ifile = os.path.join(dirname, filename)\n",
    "        if ifile.split(\".\")[-1] == \"json\":\n",
    "            datafiles.append(ifile)\n",
    "        #print(ifile)\n",
    "        \n",
    "os.mkdir(\"./files\") # location of output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affiliations(authors_json):\n",
    "    affiliations = []\n",
    "    \n",
    "    for author_meta in authors_json:\n",
    "        first = author_meta[\"first\"]\n",
    "        middle = \" \".join(author_meta[\"middle\"])\n",
    "        last = author_meta[\"last\"]\n",
    "#         print(author_meta)\n",
    "        try:\n",
    "            if author_meta[\"affiliation\"][\"institution\"] != \"\":\n",
    "                affiliation = author_meta[\"affiliation\"][\"institution\"]\n",
    "            elif author_meta[\"affiliation\"][\"laboratory\"] != \"\":\n",
    "                affiliation = author_meta[\"affiliation\"][\"laboratory\"]\n",
    "            else:\n",
    "                continue\n",
    "            affiliations.append([\" \".join([first,middle,last]), affiliation])\n",
    "        except:\n",
    "            continue\n",
    "    return affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "config = BertConfig.from_pretrained('allenai/scibert_scivocab_cased')\n",
    "config.num_labels = 3\n",
    "config.use_bfloat16 = True\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_cased', config=config)\n",
    "model = BertForSequenceClassification.from_pretrained('model.pt', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 22250\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "label_map = {0: \"Method\", 1: \"Background\", 2: \"Result\"}\n",
    "\n",
    "data = []\n",
    "\n",
    "json_files = []\n",
    "\n",
    "parsed_papers = []\n",
    "try:\n",
    "    with open(\"./files/parsed_papers.txt\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        parsed_papers = list(set(content.split(\"\\n\")))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for j,path in enumerate(tqdm(datafiles[START:])):\n",
    "    with open(path, \"r\") as infile:\n",
    "        paper_json = json.load(infile)\n",
    "\n",
    "# for paper_json in tqdm(json_files):\n",
    "    paper_id = paper_json[\"paper_id\"]\n",
    "    \n",
    "    if paper_id in parsed_papers: \n",
    "        continue\n",
    "        \n",
    "    paper_title = paper_json[\"metadata\"][\"title\"]\n",
    "    affiliations = get_affiliations(paper_json[\"metadata\"][\"authors\"])\n",
    "    bib_entries = paper_json[\"bib_entries\"]\n",
    "    \n",
    "    citations = []\n",
    "    for text_json in paper_json[\"body_text\"]:\n",
    "        if not \"cite_spans\":\n",
    "            continue\n",
    "        \n",
    "        text = text_json[\"text\"]\n",
    "        section = text_json[\"section\"]\n",
    "        sentences = sentence_splitter.split_text_into_sentences(text, language=\"en\")\n",
    "        \n",
    "        \n",
    "        for cite_span in text_json[\"cite_spans\"]:\n",
    "            cite_start = cite_span[\"start\"]\n",
    "            cite_end = cite_span[\"end\"]\n",
    "            ref_id = cite_span[\"ref_id\"]\n",
    "            \n",
    "            citing_sentence = \"\"\n",
    "            span = 0\n",
    "            for i,sent in enumerate(sentences):\n",
    "                if cite_start in range(span,span+len(sent)+1) and cite_end in range(span,span+len(sent)+1):\n",
    "                    citing_sentence = sent\n",
    "                    break\n",
    "                else:\n",
    "                    span = len(sent)\n",
    "            \n",
    "            if citing_sentence == \"\":\n",
    "                continue\n",
    "            \n",
    "            # classify intent\n",
    "            tokens = tokenizer.encode_plus(sent, truncation=True, max_length=100, padding=\"max_length\", return_tensors=\"pt\")\n",
    "            logits = model(**tokens.to(device))[0]\n",
    "            pred = torch.softmax(logits.detach().cpu(), dim=1)\n",
    "            label = int(torch.argmax(pred))\n",
    "            \n",
    "            intent = label_map[label]\n",
    "            \n",
    "            citations.append({\"ref_id\": ref_id, \n",
    "                              \"section\": section, \n",
    "                              \"intent\": intent, \n",
    "                              \"sentence\": citing_sentence})\n",
    "    \n",
    "    for i,citation in enumerate(citations):\n",
    "        ref_id = citation[\"ref_id\"]\n",
    "        \n",
    "        for k,v in bib_entries.items():\n",
    "            if k == ref_id:\n",
    "                citation[\"bib_item\"] = v\n",
    "                break\n",
    "        \n",
    "        if \"bib_item\" in citation:\n",
    "            citations[i] = citation\n",
    "        \n",
    "    data.append({\"paper_id\": paper_id, \n",
    "                 \"paper_title\": paper_title,\n",
    "                 \"citations\": citations, \n",
    "                 \"affiliations\": affiliations})\n",
    "    parsed_papers.append(paper_id)\n",
    "    \n",
    "    if j % 250 == 0:\n",
    "        with jsonlines.open(f\"./files/parsed_data_{str(START+j)}.jsonl\", \"w\") as writer:\n",
    "            writer.write(data)\n",
    "            \n",
    "        with open(f\"./files/parsed_papers_{str(START+j)}.txt\", \"a\") as f:\n",
    "            for pid in parsed_papers:\n",
    "                f.write(pid+\"\\n\")\n",
    "        data = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "de",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "de",
   "useGoogleTranslate": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
